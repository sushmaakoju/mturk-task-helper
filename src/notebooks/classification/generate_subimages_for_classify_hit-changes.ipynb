{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pillow Version: 8.1.0\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image,ImageDraw\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from math import floor\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "import urllib.request, json\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# check Pillow version number\n",
    "import PIL\n",
    "print('Pillow Version:', PIL.__version__)\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "# load and show an image with Pillow\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidelines to run this notebook\n",
    "***\n",
    "1. This file assumes you have downloaded the entire code folder from <a href=\"https://drive.google.com/drive/folders/1O8HrV5hgWwTM5dNJzhEzOaQddZSZThp5?usp=sharing\" target=\"_blank\"> this google drive folder</a>. \n",
    "\n",
    "2. This files assumes Selwyn dataset in folder above 'code' folder. So please make sure to download the LINZ datatset before proceeding to run this notebook.<a href=\"https://drive.google.com/drive/u/0/folders/1SetuO2jiS2sEx105AjiRFYyJsKk9IpGV\" target=\"_blank\"> this google drive folder.</a>.\n",
    "\n",
    "3. Please make sure to download config file in case you did not follow any of above steps <a href=\"https://drive.google.com/drive/folders/1p5eVDxIziHHWbvCaCwj-lBcrvggSsLyU?usp=sharing\" target=\"_blank\"> download config file from here.</a>.  \n",
    "\n",
    "4. And please do modify config file to add your AWS S3 folders or comment `upload_file(...)` method in `mark_image` method in below cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This file assumes you have downloaded the entire code folder from https://drive.google.com/drive/folders/1O8HrV5hgWwTM5dNJzhEzOaQddZSZThp5?usp=sharing\n"
     ]
    }
   ],
   "source": [
    "print(\"This file assumes you have downloaded the entire code folder from https://drive.google.com/drive/folders/1O8HrV5hgWwTM5dNJzhEzOaQddZSZThp5?usp=sharing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This files assumes LINZ dataset in folder above 'code' folder. So please make sure to download the LINZ datatset before proceeding to run this notebook.\n"
     ]
    }
   ],
   "source": [
    "print(\"This files assumes LINZ dataset in folder above 'code' folder. So please make sure to download the LINZ datatset before proceeding to run this notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please make sure to download config file in case you did not follow any of above steps. https://drive.google.com/drive/folders/1p5eVDxIziHHWbvCaCwj-lBcrvggSsLyU?usp=sharing \n",
      "\n",
      "And please do modify config file to add your AWS S3 folders or comment upload_file(...) method in 'mark_image' method in below cells.\n"
     ]
    }
   ],
   "source": [
    "print(\"Please make sure to download config file in case you did not follow any of above steps. https://drive.google.com/drive/folders/1p5eVDxIziHHWbvCaCwj-lBcrvggSsLyU?usp=sharing \\n\")\n",
    "print(\"And please do modify config file to add your AWS S3 folders or comment upload_file(...) method in 'mark_image' method in below cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please download config file. https://drive.google.com/drive/folders/1p5eVDxIziHHWbvCaCwj-lBcrvggSsLyU?usp=sharing \n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_folder = os.path.join(r'give_your_repository_path\\mturk-task-helper\\config')\n",
    "config_file_path = os.path.join(path, \"config.json\")\n",
    "if not os.path.exists(config_file_path) or not os.path.exists(os.path.join(path, \"config.json\")) or not os.path.exists(os.path.join(path, \"config\", \"config.json\")):\n",
    "    print(\"Please download config file. https://drive.google.com/drive/folders/1p5eVDxIziHHWbvCaCwj-lBcrvggSsLyU?usp=sharing \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defaults values\n",
    "class_thumbnail_sizes = {'bus':           171,\n",
    "                        'van_rv':        127,\n",
    "                        'small':         101,\n",
    "                        'specialized':   111,\n",
    "                        'truck':         223,\n",
    "                        'trailer_large': 219,\n",
    "                        'trailer_small': 101,\n",
    "                        'unknown':       127}\n",
    "#marker_color = (200,200,200,255)\n",
    "point_color =(0,255,0)\n",
    "bb_color =(255,0,0)\n",
    "tile_size = 400\n",
    "\n",
    "classify_images_folder = \"task1/\"\n",
    "bucketname=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = 0.2\n",
    "stride = int(tile_size * (1-overlap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload file to AWS S3 bucket\n",
    "### Upload one image to a given folder in the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(file_path:os.path, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "    print(\"S3 parameters validation\")\n",
    "    assert file_path or bucket or object_name, \"File name is empty\"\n",
    "    print(file_path, bucket, object_name)\n",
    "\n",
    "\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = file_path\n",
    "\n",
    "    # Upload the file\n",
    "    this_response = None\n",
    "    s3 = boto3.resource('s3')\n",
    "\n",
    "\n",
    "    print(\"Connect to S3 API\")\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            this_response = s3.Bucket('mturk-s3-cg').put_object(Key=object_name, Body=f,ContentType='image/png',ACL='public-read')\n",
    "        print(\"Uploading to S3 is complete!\", this_response)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate main image for alpha channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_imagefile(im):\n",
    "    #check if alpha channel exists\n",
    "    r,g,b,a = im.split()\n",
    "    assert a,\"Error: No Alpha channel for \" +im\n",
    "    image = np.asarray(im)\n",
    "    rows, columns, channels = image.shape\n",
    "    a = image[:, :, 3]\n",
    "    count = np.count_nonzero(a==0) + np.count_nonzero(a==255)\n",
    "    #print(count, (rows * columns))\n",
    "    assert (count == (rows * columns)),\"Error: Invalid values in Alpha channel for \" +image_files[0]\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# import colorsys\n",
    "# def get_contrast_marker_color(r,g,b):\n",
    "#     \"\"\" Polar Chromacity coordinates\"\"\"\n",
    "#     red, green, blue = [(255,0,0), (0,255,0), (0,0,255)]\n",
    "#     h,s,l = colorsys.rgb_to_hls(r/255.,g/255.,b/255.)\n",
    "#     #r2,g2,b2 = colorsys.hls_to_rgb(h, 1, 0.5)\n",
    "#     #h1,l1,s1 = colorsys.hls_to_rgb(r2,g2,b2)\n",
    "#     hue2 = h * 360\n",
    "#     #print(hue2)\n",
    "#     marker_color = (0,0,0)\n",
    "#     #print(hue2)\n",
    "#     if hue2 > 60 and hue2 < 180:\n",
    "#         marker_color = green\n",
    "#     elif hue2 > 180 and hue2 < 300:\n",
    "#         marker_color = blue\n",
    "#     else:\n",
    "#         marker_color = red\n",
    "#     #print(marker_color, hue2)\n",
    "    \n",
    "#     return marker_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import colorsys\n",
    "def get_marker_color(r,g,b):\n",
    "    \"\"\" HLS and Polar Chromacity coordinates\"\"\"\n",
    "    #define color ranges\n",
    "    red, green, blue, cyan, yellow, fuschia = [(255,0,0), (0,255,0), (0,0,255), (0,255,255), (255,255,0), (255,0,255)]\n",
    "    #convert to HLS\n",
    "    h,s,l = colorsys.rgb_to_hls(r/255.,g/255.,b/255.)\n",
    "    \n",
    "    #find the angle (hue2 or h2) since h from hsl is in radians\n",
    "    hue2 = (180+(h * 360))%360\n",
    "    marker_color = (0,0,0)\n",
    "    if hue2 > 0 and hue2 < 30 or hue2 > 330 and hue2 < 360:\n",
    "        marker_color = red\n",
    "    elif hue2 > 30 and hue2 <= 90:\n",
    "        marker_color = yellow\n",
    "    elif hue2 > 90 and hue2 <= 150:\n",
    "        marker_color = green\n",
    "    elif hue2 > 150 and hue2 < 210:\n",
    "        marker_color = cyan\n",
    "    elif hue2 > 210 and hue2 <=270:\n",
    "        marker_color = blue\n",
    "    else:\n",
    "        marker_color = fuschia\n",
    "    h1 = hue2/360\n",
    "    R,G,B = tuple(round(val*255) for val in colorsys.hls_to_rgb(h1, 1-l,1))\n",
    "    #print((R,G,B),marker_color)\n",
    "    return fuschia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crop corner images to remove negative numpy crop slices\n",
    "Uncmment \"#upload_file(image_file_path, bucketname, object_name)\" line in following code  \n",
    "if you want to upload to AWS S3 bucket name and AWS config keys are configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_corner_images(im:Image, xy:tuple, size:int, marker_color:tuple, image_file_path:os.path,bucketname:str, folder_name:str, upload:bool):\n",
    "    x,y = xy\n",
    "    crop = im.crop((x-size, y-size, x+size, y+size))\n",
    "    assert crop.size, \"Invalid crop size.\"\n",
    "    #plt.figure(figsize=(5, 5))\n",
    "    #plt.imshow(np.asarray(crop))\n",
    "    #print(crop.size)\n",
    "    wc,hc = crop.size\n",
    "    draw = ImageDraw.Draw(crop)\n",
    "    w1,h1 = wc//2,hc//2\n",
    "    #draw.line((w1, h1) + (w1+6,w1+6), fill=marker_color, width=1)\n",
    "    #draw.line((w1, h1+6, w1+6, h1), fill=marker_color, width=1)\n",
    "    draw.line((w1, 0)+ ( w1,hc), fill=marker_color,width=1)\n",
    "    draw.line((0, h1)+ ( wc,h1), fill=marker_color, width=1)\n",
    "    #draw.ellipse((w1+9,h1+9,w1+11, h1+11), fill=marker_color, width=3)\n",
    "    crop.convert('RGB').save(image_file_path)\n",
    "    w,h = crop.size\n",
    "    \n",
    "    #plt.figure(figsize=(5, 5))\n",
    "    #plt.imshow(np.asarray(img))\n",
    "\n",
    "    #get bucket folder and image name to upload to S3\n",
    "    object_name = folder_name+os.path.basename(image_file_path)\n",
    "    \n",
    "    #print(object_name)\n",
    "    if upload:\n",
    "        upload_file(image_file_path, bucketname, object_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crop and mark image slice\n",
    "Uncomment \"#upload_file(image_file_path, bucketname, object_name)\" line in following code  \n",
    "if you want to upload to AWS S3 bucket name and AWS config keys are configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_mark_slice(imarr:np.array, marker_color:tuple, image_file_path:os.path, bucketname:str, folder_name:str, upload:bool):\n",
    "    img = Image.fromarray(imarr)#.resize((400,400), Image.LANCZOS)\n",
    "    #img = zoom_to(img, p[0], p[1], 3)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    #draw.line((0, 0) + img.size, fill=marker_color)\n",
    "    #draw.line((0, img.size[1], img.size[0], 0), fill=marker_color)\n",
    "    \n",
    "    w,h = img.size\n",
    "\n",
    "    #print(w,h)\n",
    "    p = ( int(floor(w/2))-50, int(floor(h/2))-50, int(floor(w/2))+50, int(floor(h/2))+50 )\n",
    "    #print(p)\n",
    "\n",
    "    #draw boundingbox and point and line markers\n",
    "    #draw.rectangle(p, fill=None, outline=bb_color,width=3)\n",
    "    #draw.ellipse(( int(floor(w/2))-3, int(floor(w/2))-3, int(floor(h/2))+3, int(floor(h/2))+3 ), fill=point_color)\n",
    "    \n",
    "    w1,h1 = (w//2),(h//2)\n",
    "    draw.line((w1, 0)+ ( w1,h), fill=marker_color, width=1)\n",
    "    draw.line((0, h1)+ ( w,h1), fill=marker_color,width=1)\n",
    "    \n",
    "    #small plus sign marker\n",
    "    #draw.line((w1, h1) + (w1+6,h1+6), fill=marker_color)\n",
    "    #draw.line((w1, h1+6, w1+6, h1), fill=marker_color)\n",
    "    \n",
    "    img.convert('RGB').save(image_file_path)\n",
    "    w,h = img.size\n",
    "    \n",
    "    #plt.figure(figsize=(5, 5))\n",
    "    #plt.imshow(np.asarray(img))\n",
    "\n",
    "    #get bucket folder and image name to upload to S3\n",
    "    object_name = folder_name+os.path.basename(image_file_path)\n",
    "    \n",
    "    #print(object_name)\n",
    "    if upload:\n",
    "        upload_file(image_file_path, bucketname, object_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images_from_annotations(im, annotationsfile, outputdirname:str, cropsize:int, classify_images_folder:str, upload:bool, s3_url:str):\n",
    "    imarray = np.asarray(im)\n",
    "    assert imarray.shape, \"Image from S3 bucket url could not be opened.\"\n",
    "    #print(imarray.shape)\n",
    "    size = int(cropsize/2)\n",
    "    classes = {'small':'Small vehicles', 'bus':'Buses', 'specialized':'Specialized vehicles', 'trailer_large':'Large trailers', 'trailer_small':'Small trailers', 'truck':'Trucks', 'van_rv':'Vans and RVs', 'unknown':'Unknown vehicles', 'Not relevant vehicles':'Not relevant vehicles'}\n",
    "\n",
    "    foldername = str(os.path.splitext(os.path.basename(annotationsfile))[0].replace(\"_annotations\",\"\"))\n",
    "    #print(foldername)\n",
    "    \n",
    "    this_folder_filenames = []\n",
    "    \n",
    "    path =  os.getcwd()\n",
    "    outputdirpath = os.path.join(path, outputdirname)\n",
    "    if not os.path.exists(os.path.join(path, outputdirname)):\n",
    "        os.mkdir(outputdirpath)\n",
    "    p = os.path.join(outputdirpath, foldername)\n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir(p)\n",
    "        print(\"Directory \" , foldername ,  \" Created \") \n",
    "    except FileExistsError:\n",
    "        print(\"Directory \" , foldername ,  \" already exists\")\n",
    "        \n",
    "    # *******************Uncomment following lines to see main image preview ***********************    \n",
    "    #plot Image\n",
    "    #plt.figure(figsize=(5, 5))\n",
    "    #plt.imshow(imarray)\n",
    "    with open(annotationsfile) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    assert data, \"Invalid annotationsFile \"+ str(annotationsfile)\n",
    "    \n",
    "    #print(data['locations'])\n",
    "    for key, values in data['locations'].items():        \n",
    "        #print(key, size)\n",
    "        if key!=\"unknown\":\n",
    "            if len(values) != 0:\n",
    "                for value in values:\n",
    "                    x = int(value['x'])\n",
    "                    y = int(value['y'])\n",
    "                    \n",
    "                    #validate crop \n",
    "                    crop = im.crop((x-size, y-size, x+size, y+size))\n",
    "                    assert crop.size, \"Invalid crop \"+str(crop.size)\n",
    "                    \n",
    "                    W,H = im.size\n",
    "                    \n",
    "                    filename = foldername+\"_\"+str(x)+\"-\"+str(y)+\".png\"\n",
    "                    #print(filename, W,H,classes[key], int(value['x']),int(value['y']))\n",
    "                    img_url = \"\"\n",
    "                    if upload:\n",
    "                        img_url = s3_url+\"task1/\"+foldername+\"/\"+filename\n",
    "                    this_folder_filenames.append({\"foldername\":foldername, \"W\" : str(W),\"H\": str(H), \"x\": int(value['x']), \"y\":int(value['y']),\"class\": classes[key], \"image_url\":img_url})\n",
    "                    path = os.path.join(p, filename)\n",
    "                    #get numpy array slice\n",
    "                    crop_slice = np.s_[x-size:x+size, y-size:y+size]\n",
    "\n",
    "                    #print(crop_slice, imarray.shape)\n",
    "\n",
    "                    # get pixel color if you prefer contrast color\n",
    "                    #r,g,b,a = im.getpixel((y,x))\n",
    "                    #validate slice\n",
    "                    marker_color = (200,200,200,255)\n",
    "                    needs_padding = False\n",
    "                    \n",
    "                    # slice maybe for x,y locations around corners \n",
    "                    for slic in crop_slice:\n",
    "                        if slic.start < 0 or (slic.stop-slic.start) < (2*size):\n",
    "                            needs_padding = True\n",
    "                            break\n",
    "\n",
    "                    # either of slice locations are negative\n",
    "                    if needs_padding:\n",
    "                        crop_corner_images(im, (y,x), size, marker_color,path, bucketname, classify_images_folder+foldername+\"/\", upload)\n",
    "                    # also slice may still be valid but less than crop size\n",
    "                    else:\n",
    "                        imarr = imarray[crop_slice]\n",
    "                        h,w,_ = imarr.shape\n",
    "                        \n",
    "                        # less than crop size - use pillow crop\n",
    "                        if h < cropsize or w < cropsize:\n",
    "                            crop_corner_images(im, (y,x), size, marker_color,path, bucketname, classify_images_folder+foldername+\"/\", upload)\n",
    "                        \n",
    "                        #use numpy slice asis\n",
    "                        else:\n",
    "                            crop_mark_slice(imarray[crop_slice],marker_color, path, bucketname, classify_images_folder+foldername+\"/\", upload)\n",
    "    return this_folder_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case if you are running this notebook, and if you do not have S3 configuration setup locally on your desktop/laptop, please comment upload_file method in mark_image method in below cell. Otherwise, do change bucketname, target folder name for classify task in config file i.e. https://drive.google.com/file/d/1LeAwCJ8JEtePZiSqvF1_FmmBefbdp3pR/view?usp=sharing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'overlap': 0.2, 'tile_size': 400, 'class_thumbnail_sizes': {'bus': 171, 'van_rv': 127, 'small': 101, 'specialized': 111, 'truck': 223, 'trailer_large': 219, 'trailer_small': 101, 'unknown': 127}, 'outline_marker_color': [200, 200, 200, 255], 'point_marker_color': [255, 0, 0], 'bb_color': [255, 0, 0], 'bucketname': 'mturk-s3-cg', 'classify_images_folder': 'test_folder/task1/', 'bb_images_folder': 'test_folder/task2/', 's3_url': 'https://mturk-s3-cg.s3.amazonaws.com/'}\n"
     ]
    }
   ],
   "source": [
    "path = Path(os.getcwd())\n",
    "#get new folder's storage entry folder, there on, folders likely similar ?\n",
    "filedir = os.path.join(r'give_your_repository_path\\linz\\LINZ\\Final\\001_selwyn-0125m-urban-aerial-photos-2012-2013\\0001') # example\n",
    "                       #\"0001\",\"0001\",\"01 Final Annotation Galleries\"\n",
    "    \n",
    "main_folders = [os.path.join(filedir,name) for name in os.listdir(filedir)]\n",
    "results = dict()\n",
    "this_directory = None\n",
    "annotations_count = 0\n",
    "file_class_count = dict()\n",
    "\n",
    "#load json config\n",
    "config_file_path = os.path.join(r'give_your_repository_path\\mturk-task-helper\\config\\config_example.json')\n",
    "if os.path.exists(config_file_path) :\n",
    "    with open(config_file_path, \"r\") as jsonfile:\n",
    "        config = json.load(jsonfile)\n",
    "        print(config)\n",
    "        assert config, \"Config JSON has errors or is empty.\"\n",
    "\n",
    "class_thumbnail_sizes = config[\"class_thumbnail_sizes\"]\n",
    "\n",
    "#marker_color = tuple(config[\"marker_color\"])\n",
    "point_color = tuple(config[\"point_marker_color\"])\n",
    "bb_color = config[\"bb_color\"]\n",
    "tile_size = config[\"tile_size\"]\n",
    "overlap = config[\"overlap\"]\n",
    "stride = int(tile_size * (1-overlap))\n",
    "bucketname = config[\"bucketname\"]\n",
    "classify_images_folder = config[\"classify_images_folder\"]\n",
    "s3_url = config['s3_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is an example for generating slices with 300, 400 and 500 \n",
    "len(main_folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following cell walks through given folder in this I am only using single folder for one single main Image folder for generating images for 300, 400 and 500 respectively:\n",
    "The example block of code -> extract_images_from_annotations(im,annotationsfile, \"temp2_300\", 300, ,classify_images_folder+\"examples\"+\"/\"+\"300\"+\"/\") slices subimages to 300*300 and generates cropped images and uploads to respective classifyImages folder \n",
    "Note that on AWS S3 storage I already created 3 folders \"300\", \"400\" and \"500\" respectively. In any case if you are running this notebook, and if you do not have S3 configuration setup locally on your desktop/laptop, please comment upload_file method in mark_image method. above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  0001_0001  Created \n",
      "\n"
     ]
    }
   ],
   "source": [
    "classify_task_answers = []\n",
    "\n",
    "for f in main_folders[:1]:\n",
    "    this_key = None\n",
    "    for root, dir, files in os.walk(f):\n",
    "        #ignore ipynb_checkpoints folders\n",
    "        if re.search(\".ipynb_checkpoints\",os.path.basename(Path(root))):\n",
    "            continue\n",
    "        if len(dir) != 0:\n",
    "            #check if Annotation galleries exist\n",
    "            galleries = [ d for d in dir if re.search(\"\\d+\\sFinal Annotation Galleries\",str(d))]\n",
    "            #check if main image file exists 000X_000X_image.png exists ?\n",
    "            image_files = [ file for file in files if re.search(\"\\d+(_)\\d+(_)image.png\",file)]\n",
    "            annotation_files = [ file for file in files if re.search(\"\\d+(_)\\d+(_)annotations.json\",file)]\n",
    "        #if dir is empty, we continue\n",
    "        #valid folders and files exists ?\n",
    "        if len(galleries) > 0 and len(image_files) > 0 and image_files[0] and \"0001_0001\" in image_files[0] :\n",
    "            #print(root)\n",
    "            imagefilepath = Path(root).joinpath( image_files[0])\n",
    "            #print(imagefilepath)\n",
    "            annotationsfile = Path(root, annotation_files[0])\n",
    "            im = Image.open(imagefilepath)\n",
    "            #valid_image = validate_imagefile(im)\n",
    "            \n",
    "            #if invalid image, Assertion Errors are raised so this never moves to next statement\n",
    "            this_image_classify_task_answers = extract_images_from_annotations(im,annotationsfile, \"task1\", 306,classify_images_folder+\"examples/new\"+\"/\"+\"300\"+\"/\", False, s3_url)\n",
    "            print(this_image_classify_task_answers[0]['image_url'])\n",
    "            classify_task_answers.extend(this_image_classify_task_answers)\n",
    "            \n",
    "            #if you want multiple sub-image sizes to be generated at a time for each main image\n",
    "            #extract_images_from_annotations(im,annotationsfile, \"temp3_400\", 400,classify_images_folder+\"examples/new\"+\"/\"+\"400\"+\"/\", False, s3_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classify_task_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate cv file with answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result = pd.DataFrame.from_dict(classify_task_answers)\n",
    "result.to_csv(os.path.join(os.getcwd(),\"selwyn_answers_classification_task1_test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a different format of same answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wh ('24174', '11141') 11\n"
     ]
    }
   ],
   "source": [
    "classes = {'small':'Small vehicles', 'bus':'Buses', 'specialized':'Specialized vehicles', 'trailer_large':'Large trailers', 'trailer_small':'Small trailers', 'truck':'Trucks', 'van_rv':'Vans and RVs', 'unknown':'Unknown vehicles', 'Not relevant vehicles':'Not relevant vehicles'}\n",
    "all_answers = []\n",
    "for f in main_folders[0:1]:\n",
    "    this_key = None\n",
    "    for root, dir, files in os.walk(f):\n",
    "        #ignore ipynb_checkpoints folders\n",
    "        if re.search(\".ipynb_checkpoints\",os.path.basename(Path(root))):\n",
    "            continue\n",
    "        if len(dir) != 0:\n",
    "            #check if Annotation galleries exist\n",
    "            galleries = [ d for d in dir if re.search(\"\\d+\\sFinal Annotation Galleries\",str(d))]\n",
    "            #check if main image file exists 000X_000X_image.png exists ?\n",
    "            image_files = [ file for file in files if re.search(\"\\d+(_)\\d+(_)image.png\",file)]\n",
    "            annotation_files = [ file for file in files if re.search(\"\\d+(_)\\d+(_)annotations.json\",file)]\n",
    "        #if dir is empty, we continue\n",
    "        #valid folders and files exists ?\n",
    "        if len(galleries) > 0 and len(image_files) > 0 and image_files[0] :\n",
    "            #print(root)\n",
    "            imagefilepath = Path(root).joinpath( image_files[0])\n",
    "            #print(imagefilepath)\n",
    "            annotationsfile = Path(root, annotation_files[0])\n",
    "            im = Image.open(imagefilepath)\n",
    "            w,h = im.size\n",
    "            #print((w,h),image_files[0], foldername )\n",
    "            with open(annotationsfile) as json_file:\n",
    "                data = json.load(json_file)\n",
    "            for key, values in data['locations'].items():\n",
    "                if len(values) != 0:\n",
    "                    for value in values:\n",
    "                        x = int(value['x'])\n",
    "                        y = int(value['y'])\n",
    "                        foldername = str(os.path.splitext(os.path.basename(annotationsfile))[0].replace(\"_annotations\",\"\"))\n",
    "                        filename = foldername+\"_\"+str(x)+\"-\"+str(y)+\".png\"\n",
    "                        k = classes[key]\n",
    "                        #print(k)\n",
    "                        all_answers.append({\"image_name\":image_files[0], \"W\" : str(w),\"H\": str(h), \"x\": int(value['x']), \"y\":int(value['y']),\"class\": classes[key], \"image_url\":\"https://mturk-s3-cg.s3.amazonaws.com/\"+\"task1/batch100/\"+foldername+\"/\"+filename})\n",
    "            print(\"wh\" , (str(w), str(h)),len(all_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
